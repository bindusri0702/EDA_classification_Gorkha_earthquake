{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802906</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28830</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94947</td>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590882</td>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201944</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "0       802906               6             487           12198   \n",
       "1        28830               8             900            2812   \n",
       "2        94947              21             363            8973   \n",
       "3       590882              22             418           10694   \n",
       "4       201944              11             131            1488   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "0                    2   30                6                  5   \n",
       "1                    2   10                8                  7   \n",
       "2                    2   10                5                  5   \n",
       "3                    2   10                6                  5   \n",
       "4                    3   30                8                  9   \n",
       "\n",
       "  land_surface_condition foundation_type  ... has_secondary_use_agriculture  \\\n",
       "0                      t               r  ...                             0   \n",
       "1                      o               r  ...                             0   \n",
       "2                      t               r  ...                             0   \n",
       "3                      t               r  ...                             0   \n",
       "4                      t               r  ...                             0   \n",
       "\n",
       "  has_secondary_use_hotel has_secondary_use_rental  \\\n",
       "0                       0                        0   \n",
       "1                       0                        0   \n",
       "2                       0                        0   \n",
       "3                       0                        0   \n",
       "4                       0                        0   \n",
       "\n",
       "  has_secondary_use_institution has_secondary_use_school  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "\n",
       "   has_secondary_use_industry  has_secondary_use_health_post  \\\n",
       "0                           0                              0   \n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   has_secondary_use_gov_office  has_secondary_use_use_police  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   has_secondary_use_other  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\bindu\\Desktop\\nepal_earthquake\\train_values.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802905</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94946</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201943</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  damage_grade\n",
       "0       802905             2\n",
       "1        28829             1\n",
       "2        94946             2\n",
       "3       590881             1\n",
       "4       201943             2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(r\"C:\\Users\\bindu\\Desktop\\nepal_earthquake\\train_labels.csv\")\n",
    "labels = labels-1\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300051</td>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99355</td>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890251</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745817</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>421793</td>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "0       300051              17             596           11307   \n",
       "1        99355               6             141           11987   \n",
       "2       890251              22              19           10044   \n",
       "3       745817              26              39             633   \n",
       "4       421793              17             289            7970   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "0                    3   20                7                  6   \n",
       "1                    2   25               13                  5   \n",
       "2                    2    5                4                  5   \n",
       "3                    1    0               19                  3   \n",
       "4                    3   15                8                  7   \n",
       "\n",
       "  land_surface_condition foundation_type  ... has_secondary_use_agriculture  \\\n",
       "0                      t               r  ...                             0   \n",
       "1                      t               r  ...                             1   \n",
       "2                      t               r  ...                             0   \n",
       "3                      t               r  ...                             0   \n",
       "4                      t               r  ...                             0   \n",
       "\n",
       "  has_secondary_use_hotel has_secondary_use_rental  \\\n",
       "0                       0                        0   \n",
       "1                       0                        0   \n",
       "2                       0                        0   \n",
       "3                       0                        1   \n",
       "4                       0                        0   \n",
       "\n",
       "  has_secondary_use_institution has_secondary_use_school  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "\n",
       "   has_secondary_use_industry  has_secondary_use_health_post  \\\n",
       "0                           0                              0   \n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   has_secondary_use_gov_office  has_secondary_use_use_police  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   has_secondary_use_other  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(r\"C:\\Users\\bindu\\Desktop\\nepal_earthquake\\test_values.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.merge(labels,left_on= 'building_id',right_on='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['land_surface_condition','foundation_type','roof_type',\n",
    "                                        'legal_ownership_status','ground_floor_type','other_floor_type','position','plan_configuration'] #'legal_ownership_status',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    train[f'{col}_encoded'] = train[col].astype('category').cat.codes\n",
    "\n",
    "for col in categorical_columns:\n",
    "    test_df[f'{col}_encoded'] = test_df[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=categorical_columns,inplace=True)\n",
    "test_df.drop(columns=categorical_columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input (batch_size=5, geo_level_1_id, geo_level_2_id, geo_level_3_id)\n",
    "input_geo_data = torch.tensor(train[['geo_level_1_id','geo_level_2_id','geo_level_3_id']].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Sample labels for damage grades (output classes, e.g., damage grade 1, 2, 3)\n",
    "labels = torch.tensor(train['damage_grade'].to_numpy(), dtype=torch.long)\n",
    "\n",
    "train.drop(columns='damage_grade',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = torch.tensor([3.45, 0.59, 0.99])  \n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#{1: 3.441821852417879, 2: 0.5856029379662802, 3: 0.9981887924005888}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.2209\n",
      "Epoch [2/100], Loss: 1.2148\n",
      "Epoch [3/100], Loss: 1.2088\n",
      "Epoch [4/100], Loss: 1.2030\n",
      "Epoch [5/100], Loss: 1.1973\n",
      "Epoch [6/100], Loss: 1.1917\n",
      "Epoch [7/100], Loss: 1.1863\n",
      "Epoch [8/100], Loss: 1.1810\n",
      "Epoch [9/100], Loss: 1.1758\n",
      "Epoch [10/100], Loss: 1.1708\n",
      "Epoch [11/100], Loss: 1.1658\n",
      "Epoch [12/100], Loss: 1.1610\n",
      "Epoch [13/100], Loss: 1.1563\n",
      "Epoch [14/100], Loss: 1.1517\n",
      "Epoch [15/100], Loss: 1.1473\n",
      "Epoch [16/100], Loss: 1.1429\n",
      "Epoch [17/100], Loss: 1.1387\n",
      "Epoch [18/100], Loss: 1.1345\n",
      "Epoch [19/100], Loss: 1.1305\n",
      "Epoch [20/100], Loss: 1.1266\n",
      "Epoch [21/100], Loss: 1.1228\n",
      "Epoch [22/100], Loss: 1.1191\n",
      "Epoch [23/100], Loss: 1.1155\n",
      "Epoch [24/100], Loss: 1.1120\n",
      "Epoch [25/100], Loss: 1.1086\n",
      "Epoch [26/100], Loss: 1.1052\n",
      "Epoch [27/100], Loss: 1.1020\n",
      "Epoch [28/100], Loss: 1.0988\n",
      "Epoch [29/100], Loss: 1.0958\n",
      "Epoch [30/100], Loss: 1.0928\n",
      "Epoch [31/100], Loss: 1.0898\n",
      "Epoch [32/100], Loss: 1.0870\n",
      "Epoch [33/100], Loss: 1.0842\n",
      "Epoch [34/100], Loss: 1.0815\n",
      "Epoch [35/100], Loss: 1.0788\n",
      "Epoch [36/100], Loss: 1.0763\n",
      "Epoch [37/100], Loss: 1.0737\n",
      "Epoch [38/100], Loss: 1.0712\n",
      "Epoch [39/100], Loss: 1.0688\n",
      "Epoch [40/100], Loss: 1.0664\n",
      "Epoch [41/100], Loss: 1.0641\n",
      "Epoch [42/100], Loss: 1.0618\n",
      "Epoch [43/100], Loss: 1.0596\n",
      "Epoch [44/100], Loss: 1.0574\n",
      "Epoch [45/100], Loss: 1.0553\n",
      "Epoch [46/100], Loss: 1.0532\n",
      "Epoch [47/100], Loss: 1.0511\n",
      "Epoch [48/100], Loss: 1.0490\n",
      "Epoch [49/100], Loss: 1.0470\n",
      "Epoch [50/100], Loss: 1.0451\n",
      "Epoch [51/100], Loss: 1.0431\n",
      "Epoch [52/100], Loss: 1.0412\n",
      "Epoch [53/100], Loss: 1.0393\n",
      "Epoch [54/100], Loss: 1.0375\n",
      "Epoch [55/100], Loss: 1.0357\n",
      "Epoch [56/100], Loss: 1.0339\n",
      "Epoch [57/100], Loss: 1.0321\n",
      "Epoch [58/100], Loss: 1.0303\n",
      "Epoch [59/100], Loss: 1.0286\n",
      "Epoch [60/100], Loss: 1.0269\n",
      "Epoch [61/100], Loss: 1.0252\n",
      "Epoch [62/100], Loss: 1.0236\n",
      "Epoch [63/100], Loss: 1.0219\n",
      "Epoch [64/100], Loss: 1.0203\n",
      "Epoch [65/100], Loss: 1.0187\n",
      "Epoch [66/100], Loss: 1.0172\n",
      "Epoch [67/100], Loss: 1.0156\n",
      "Epoch [68/100], Loss: 1.0141\n",
      "Epoch [69/100], Loss: 1.0126\n",
      "Epoch [70/100], Loss: 1.0111\n",
      "Epoch [71/100], Loss: 1.0096\n",
      "Epoch [72/100], Loss: 1.0082\n",
      "Epoch [73/100], Loss: 1.0067\n",
      "Epoch [74/100], Loss: 1.0053\n",
      "Epoch [75/100], Loss: 1.0039\n",
      "Epoch [76/100], Loss: 1.0025\n",
      "Epoch [77/100], Loss: 1.0012\n",
      "Epoch [78/100], Loss: 0.9998\n",
      "Epoch [79/100], Loss: 0.9985\n",
      "Epoch [80/100], Loss: 0.9971\n",
      "Epoch [81/100], Loss: 0.9958\n",
      "Epoch [82/100], Loss: 0.9945\n",
      "Epoch [83/100], Loss: 0.9932\n",
      "Epoch [84/100], Loss: 0.9920\n",
      "Epoch [85/100], Loss: 0.9907\n",
      "Epoch [86/100], Loss: 0.9895\n",
      "Epoch [87/100], Loss: 0.9883\n",
      "Epoch [88/100], Loss: 0.9870\n",
      "Epoch [89/100], Loss: 0.9858\n",
      "Epoch [90/100], Loss: 0.9847\n",
      "Epoch [91/100], Loss: 0.9835\n",
      "Epoch [92/100], Loss: 0.9823\n",
      "Epoch [93/100], Loss: 0.9812\n",
      "Epoch [94/100], Loss: 0.9800\n",
      "Epoch [95/100], Loss: 0.9789\n",
      "Epoch [96/100], Loss: 0.9778\n",
      "Epoch [97/100], Loss: 0.9767\n",
      "Epoch [98/100], Loss: 0.9756\n",
      "Epoch [99/100], Loss: 0.9745\n",
      "Epoch [100/100], Loss: 0.9734\n"
     ]
    }
   ],
   "source": [
    "class GeospatialEmbeddingModel(nn.Module):\n",
    "    def __init__(self, geo_lv1_size, geo_lv2_size, geo_lv3_size, latent_dim):\n",
    "        super(GeospatialEmbeddingModel, self).__init__()\n",
    "        # Embedding layers\n",
    "        self.geo_level1_embedding = nn.Embedding(geo_lv1_size, 16)\n",
    "        self.geo_level2_embedding = nn.Embedding(geo_lv2_size, 128)\n",
    "        self.geo_level3_embedding = nn.Embedding(geo_lv3_size, 128)\n",
    "\n",
    "        # Initialize embeddings\n",
    "        nn.init.uniform_(self.geo_level1_embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.geo_level2_embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.geo_level3_embedding.weight, -0.1, 0.1)\n",
    "\n",
    "        # Compressor layer\n",
    "        self.compressor = nn.Linear(16 + 128 + 128, latent_dim)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(latent_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(latent_dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        geo1_embedded = self.geo_level1_embedding(x[:, 0])\n",
    "        geo2_embedded = self.geo_level2_embedding(x[:, 1])\n",
    "        geo3_embedded = self.geo_level3_embedding(x[:, 2])\n",
    "\n",
    "        concatenated = torch.cat([geo1_embedded, geo2_embedded, geo3_embedded], dim=1)\n",
    "        compressed = torch.relu(self.compressor(concatenated))\n",
    "        compressed = self.batch_norm(compressed)\n",
    "        output = self.output(compressed)\n",
    "        return output\n",
    "\n",
    "# Example data sizes\n",
    "geo_level_1_size, geo_level_2_size, geo_level_3_size = 31, 1428, 12568\n",
    "latent_dim = 16\n",
    "\n",
    "# Instantiate model\n",
    "embds_model = GeospatialEmbeddingModel(geo_level_1_size, geo_level_2_size, geo_level_3_size, latent_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([3.45, 0.59, 0.99]))  # Optionally, apply class weights\n",
    "optimizer = optim.Adam(embds_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Train for enough epochs\n",
    "for epoch in range(num_epochs):\n",
    "    #for inputs, labels in data_loader:  # Assume data_loader is defined\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs = embds_model(input_geo_data)  # Forward pass\n",
    "    loss = criterion(outputs, labels)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_2 = train.iloc[:,4:].to_numpy()\n",
    "final_1 = np.hstack((outputs.detach().numpy(), train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64854, 38)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feed-forward neural network\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.linear_relu_norm = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),        # Normalize the layer\n",
    "            nn.Dropout(0.3),           # Dropout layer to prevent overfitting\n",
    "            nn.Linear(32, 16),         # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)  # Output layer\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_norm(x)\n",
    "        return logits\n",
    "\n",
    "# Example: Input size is 10, hidden layer size is 64, output i\n",
    "# s 3 classes\n",
    "input_dim = final_1.shape[1]\n",
    "\n",
    "classfication_model = ClassificationModel(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1752\n",
      "Epoch [2/100], Loss: 1.1761\n",
      "Epoch [3/100], Loss: 1.1748\n",
      "Epoch [4/100], Loss: 1.1745\n",
      "Epoch [5/100], Loss: 1.1747\n",
      "Epoch [6/100], Loss: 1.1739\n",
      "Epoch [7/100], Loss: 1.1727\n",
      "Epoch [8/100], Loss: 1.1726\n",
      "Epoch [9/100], Loss: 1.1718\n",
      "Epoch [10/100], Loss: 1.1718\n",
      "Epoch [11/100], Loss: 1.1709\n",
      "Epoch [12/100], Loss: 1.1705\n",
      "Epoch [13/100], Loss: 1.1691\n",
      "Epoch [14/100], Loss: 1.1691\n",
      "Epoch [15/100], Loss: 1.1684\n",
      "Epoch [16/100], Loss: 1.1686\n",
      "Epoch [17/100], Loss: 1.1678\n",
      "Epoch [18/100], Loss: 1.1666\n",
      "Epoch [19/100], Loss: 1.1669\n",
      "Epoch [20/100], Loss: 1.1661\n",
      "Epoch [21/100], Loss: 1.1653\n",
      "Epoch [22/100], Loss: 1.1648\n",
      "Epoch [23/100], Loss: 1.1643\n",
      "Epoch [24/100], Loss: 1.1643\n",
      "Epoch [25/100], Loss: 1.1634\n",
      "Epoch [26/100], Loss: 1.1628\n",
      "Epoch [27/100], Loss: 1.1633\n",
      "Epoch [28/100], Loss: 1.1627\n",
      "Epoch [29/100], Loss: 1.1618\n",
      "Epoch [30/100], Loss: 1.1614\n",
      "Epoch [31/100], Loss: 1.1607\n",
      "Epoch [32/100], Loss: 1.1610\n",
      "Epoch [33/100], Loss: 1.1600\n",
      "Epoch [34/100], Loss: 1.1590\n",
      "Epoch [35/100], Loss: 1.1585\n",
      "Epoch [36/100], Loss: 1.1586\n",
      "Epoch [37/100], Loss: 1.1579\n",
      "Epoch [38/100], Loss: 1.1581\n",
      "Epoch [39/100], Loss: 1.1573\n",
      "Epoch [40/100], Loss: 1.1565\n",
      "Epoch [41/100], Loss: 1.1566\n",
      "Epoch [42/100], Loss: 1.1552\n",
      "Epoch [43/100], Loss: 1.1555\n",
      "Epoch [44/100], Loss: 1.1546\n",
      "Epoch [45/100], Loss: 1.1546\n",
      "Epoch [46/100], Loss: 1.1538\n",
      "Epoch [47/100], Loss: 1.1529\n",
      "Epoch [48/100], Loss: 1.1525\n",
      "Epoch [49/100], Loss: 1.1525\n",
      "Epoch [50/100], Loss: 1.1518\n",
      "Epoch [51/100], Loss: 1.1512\n",
      "Epoch [52/100], Loss: 1.1519\n",
      "Epoch [53/100], Loss: 1.1514\n",
      "Epoch [54/100], Loss: 1.1507\n",
      "Epoch [55/100], Loss: 1.1502\n",
      "Epoch [56/100], Loss: 1.1488\n",
      "Epoch [57/100], Loss: 1.1488\n",
      "Epoch [58/100], Loss: 1.1475\n",
      "Epoch [59/100], Loss: 1.1483\n",
      "Epoch [60/100], Loss: 1.1476\n",
      "Epoch [61/100], Loss: 1.1467\n",
      "Epoch [62/100], Loss: 1.1465\n",
      "Epoch [63/100], Loss: 1.1467\n",
      "Epoch [64/100], Loss: 1.1457\n",
      "Epoch [65/100], Loss: 1.1451\n",
      "Epoch [66/100], Loss: 1.1450\n",
      "Epoch [67/100], Loss: 1.1449\n",
      "Epoch [68/100], Loss: 1.1439\n",
      "Epoch [69/100], Loss: 1.1439\n",
      "Epoch [70/100], Loss: 1.1425\n",
      "Epoch [71/100], Loss: 1.1432\n",
      "Epoch [72/100], Loss: 1.1427\n",
      "Epoch [73/100], Loss: 1.1427\n",
      "Epoch [74/100], Loss: 1.1405\n",
      "Epoch [75/100], Loss: 1.1408\n",
      "Epoch [76/100], Loss: 1.1404\n",
      "Epoch [77/100], Loss: 1.1397\n",
      "Epoch [78/100], Loss: 1.1402\n",
      "Epoch [79/100], Loss: 1.1394\n",
      "Epoch [80/100], Loss: 1.1390\n",
      "Epoch [81/100], Loss: 1.1387\n",
      "Epoch [82/100], Loss: 1.1379\n",
      "Epoch [83/100], Loss: 1.1372\n",
      "Epoch [84/100], Loss: 1.1372\n",
      "Epoch [85/100], Loss: 1.1369\n",
      "Epoch [86/100], Loss: 1.1369\n",
      "Epoch [87/100], Loss: 1.1363\n",
      "Epoch [88/100], Loss: 1.1356\n",
      "Epoch [89/100], Loss: 1.1352\n",
      "Epoch [90/100], Loss: 1.1352\n",
      "Epoch [91/100], Loss: 1.1349\n",
      "Epoch [92/100], Loss: 1.1338\n",
      "Epoch [93/100], Loss: 1.1335\n",
      "Epoch [94/100], Loss: 1.1333\n",
      "Epoch [95/100], Loss: 1.1329\n",
      "Epoch [96/100], Loss: 1.1316\n",
      "Epoch [97/100], Loss: 1.1320\n",
      "Epoch [98/100], Loss: 1.1313\n",
      "Epoch [99/100], Loss: 1.1301\n",
      "Epoch [100/100], Loss: 1.1304\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Optionally, apply class weights\n",
    "optimizer = optim.Adam(classfication_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Train for enough epochs\n",
    "for epoch in range(num_epochs):\n",
    "    #for inputs, labels in data_loader:  # Assume data_loader is defined\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    logits = classfication_model(torch.tensor(final_1,dtype=torch.float32))  # Forward pass\n",
    "    loss = criterion(logits, labels)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_geo_data2 = torch.tensor(test_df[['geo_level_1_id','geo_level_2_id','geo_level_3_id']].to_numpy(), dtype=torch.long)\n",
    "outputs2 = embds_model(input_geo_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test_df.iloc[:,4:].to_numpy()\n",
    "final_2 = np.hstack((outputs2.detach().numpy(), test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868, 38)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = classfication_model(torch.tensor(final_2,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(test_logits)\n",
    "y_pred = pred_probab.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 65468, 0: 20447, 1: 953})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame({'building_id' : test_df['building_id'],'damage_grade' : y_pred+1})\n",
    "sub2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2.to_csv(\"geo_embdngs_nn_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(outputs2.detach().numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(len(labels)):\n",
    "    if preds[i] + 1 != labels[i]:\n",
    "        cnt += 1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(outputs, 'output_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensor = torch.load('output_tensor.pt')\n",
    "loaded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_embds = loaded_tensor.detach().numpy()\n",
    "geo_embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mtrx = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=[20,20])\n",
    "sns.heatmap(corr_mtrx,annot=True,cmap='Reds')\n",
    "plt.title(\"Correlation between Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test_df.iloc[:,4:].to_numpy()\n",
    "geo_embds_2 = outputs2.detach().numpy()\n",
    "final_2 = np.hstack((geo_embds_2, test_2))\n",
    "final_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = y\n",
    "X = final_1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                     classes=np.unique(y_train), \n",
    "                                     y=y_train)\n",
    "class_weight_dict = {}\n",
    "for i in range(1,len(class_weights)+1):\n",
    "    class_weight_dict[i] = class_weights[i-1]\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train model\n",
    "model = RandomForestClassifier(n_estimators=100,min_samples_leaf=5 )#, class_weight=class_weight_dict)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'multi:softmax',\n",
    "            'max_depth': 6,\n",
    "            'n_estimators':100,\n",
    "            'num_classes':3,\n",
    "            'eval_metric' : 'merror' \n",
    "        }\n",
    "            \n",
    "            \n",
    "            \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X,y-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "y_pred = xgb_clf.predict(final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro = f1_score(y_test-1, y_pred, average='micro')\n",
    "f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2.to_csv(\"geo_embdngs_xgb_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legal_ownership_status\n",
    "# land_surface_condition  \n",
    "# foundation_type         \n",
    "# roof_type               \n",
    "# ground_floor_type       \n",
    "# other_floor_type        \n",
    "# position                \n",
    "# plan_configuration          \n",
    "set(train_df['plan_configuration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "xx = Counter(train['geo_level_1_id'])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'damage_grade' and sum or average 'count_floors_pre_eq'\n",
    "geo_damage = train.groupby(['geo_level_1_id','damage_grade'])['building_id'].count().reset_index()\n",
    "geo_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_geo_damage = geo_damage.loc[geo_damage.groupby('geo_level_1_id')['building_id'].idxmax()]\n",
    "max_geo_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_geo_damage[max_geo_damage['damage_grade'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'damage_grade' and sum or average 'count_floors_pre_eq'\n",
    "geo_damage2 = train.groupby(['geo_level_2_id','damage_grade'])['building_id'].count().reset_index()\n",
    "geo_damage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_geo_damage2 = geo_damage2.loc[geo_damage2.groupby('geo_level_2_id')['building_id'].idxmax()]\n",
    "max_geo_damage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_geo_damage2[max_geo_damage2['damage_grade'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo12 = train.groupby(['geo_level_1_id','geo_level_2_id'])['building_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo12[geo12['geo_level_2_id'] == 21] #8,17,18,21,27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_damage = train.groupby(['has_superstructure_rc_non_engineered','damage_grade'])['building_id'].count().reset_index()\n",
    "\n",
    "concrete_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_damage2 = train.groupby(['has_superstructure_rc_engineered','damage_grade'])['building_id'].count().reset_index()\n",
    "\n",
    "concrete_damage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_damage3 = train.groupby(['has_superstructure_other','damage_grade'])['building_id'].count().reset_index()\n",
    "\n",
    "concrete_damage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floors_damage = train.groupby(['count_floors_pre_eq','damage_grade'])['building_id'].count().reset_index()\n",
    "\n",
    "floors_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=floors_damage, kind=\"bar\",\n",
    "    x=\"count_floors_pre_eq\", y=\"building_id\", hue=\"damage_grade\",\n",
    "    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Number of floors\", \"Count of buildings\")\n",
    "g.legend.set_title(\"Relation of number of floors to Damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'damage_grade' and sum or average 'count_floors_pre_eq'\n",
    "age_damage = train.groupby(['age','damage_grade'])['building_id'].count().reset_index()\n",
    "\n",
    "age_damage = age_damage[age_damage['age'] < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(age_damage['age'],age_damage['building_id'],c=age_damage['damage_grade'])\n",
    "plt.colorbar(label='Color intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['height_percentage'],train['area_percentage'],c=train['damage_grade'])\n",
    "plt.colorbar(label='Color intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'damage_grade' and sum or average 'count_floors_pre_eq'\n",
    "area_damage = train.groupby(['area_percentage','damage_grade'])['building_id'].count().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(area_damage['area_percentage'],area_damage['building_id'],c=area_damage['damage_grade'])\n",
    "plt.colorbar(label='Color intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'damage_grade' and sum or average 'count_floors_pre_eq'\n",
    "height_damage = train.groupby(['height_percentage','damage_grade'])['building_id'].count().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(height_damage['height_percentage'],height_damage['building_id'],c=height_damage['damage_grade'])\n",
    "plt.colorbar(label='Color intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['legal_ownership_status','land_surface_condition','foundation_type','roof_type',\n",
    "                                        'ground_floor_type','other_floor_type','position','plan_configuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    train[f'{col}_encoded'] = train[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=categorical_columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_values.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    test[f'{col}_encoded'] = test[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=categorical_columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = train['damage_grade']\n",
    "X = train.drop(columns = ['damage_grade'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Decision Tree Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_micro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(test)\n",
    "# print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "# f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.DataFrame({'building_id' : test['building_id'],'damage_grade' : y_pred})\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv(\"rf_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(test)\n",
    "# print(f'Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "# f1_micro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 = pd.DataFrame({'building_id' : test['building_id'],'damage_grade' : y_pred})\n",
    "sub3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3.to_csv(\"gbc_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Initialize and train model\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(test)\n",
    "# print(f'LightGBM Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "# f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame({'building_id' : test['building_id'],'damage_grade' : y_pred})\n",
    "sub2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2.to_csv(\"lgb_predictions.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extract_locations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
